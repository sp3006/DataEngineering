import os
import glob
import json
import psycopg2
import pandas as pd
import json
from sql_queries import *
print("*****OK******")


conn = psycopg2.connect("host=127.0.0.1 dbname=sparkifydb user=student password=student")
cur = conn.cursor()
conn.set_session(autocommit=True)
print (conn)


def get_files(filepath):
    all_files = []
    for root, dirs, files in os.walk(filepath):
        files = glob.glob(os.path.join(root,'*.json'))
        for f in files :
            all_files.append(os.path.abspath(f))
    
    return all_files
    
    

song_files = get_files('data/song_data')

filepath = song_files[0]

df = pd.read_json(filepath , orient='records', typ='frame', lines=True)
df.head()

/*Extract Data for Songs Table
Select columns for song ID, title, artist ID, year, and duration
Use df.values to select just the values from the dataframe
Index to select the first (only) record in the dataframe
Convert the array to a list and set it to song_data*/

song_data = df[['song_id', 'title', 'artist_id', 'year', 'duration']].values[0].flatten()
song_data = song_data.tolist()
song_data

cur.execute(song_table_insert, song_data)
conn.commit()

Extract Data for Artists Table
Select columns for artist ID, name, location, latitude, and longitude
Use df.values to select just the values from the dataframe
Index to select the first (only) record in the dataframe
Convert the array to a list and set it to artist_data

artist_data = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']].values[0].flatten()
artist_data = artist_data.tolist()
artist_data

#Insert Record into Artist TableÂ¶
#Implement the artist_table_insert query in sql_queries.py and run the cell below to insert a record for 
#this song's artist into the artists table. Remember to run create_tables.py before running the cell below to ensure 
#you've created/resetted the artists table in the sparkify database.

#Process log_data
#In this part, you'll perform ETL on the second dataset, log_data, to create the time and users dimensional tables, as well as the songplays fact table.

#Let's perform ETL on a single log file and load a single record into each table.

#Use the get_files function provided above to get a list of all log JSON files in data/log_data
#Select the first log file in this list
#Read the log file and view the data

log_files = get_files('data/log_data')

filepath = log_files[0]

df = pd.read_json(filepath, orient='records', typ='frame' ,lines=True)
df.head(1)

is_song = df["page"] == 'NextSong'
df = df[is_song]
df.head(1)

t  = pd.to_datetime(df['ts'], unit='ms')
t.head(1)

time_data = (t.dt.time,t.dt.hour,t.dt.day,t.dt.weekofyear,t.dt.month,t.dt.year,t.dt.weekday)
time_data
column_labels = ("start_time","hour","day","week","month", "year","weekday")

time_df = pd.DataFrame(dict(zip(column_labels,time_data)),columns=column_labels).reset_index(drop=True)  
time_df.head(1)

for i, row in time_df.iterrows():
    cur.execute(time_table_insert, list(row))
    conn.commit()

for i, row in user_df.iterrows():
    cur.execute(user_table_insert, row)
    conn.commit()

for index, row in df.iterrows():

    # get songid and artistid from song and artist tables
    cur.execute(song_select, (row.song, row.artist, row.length))
    print(row.song)
    print(row.artist)
    print(row.length)
    results = cur.fetchone()
    
    if results:
        songid, artistid = results
    else:
        songid, artistid = None, None

    # insert songplay record
    songplay_data = (pd.to_datetime(row.ts, unit='ms'),row.userId,row.level,songid,artistid,row.sessionId,row.location,row.userAgent)
    print(songplay_data)
    cur.execute(songplay_table_insert, songplay_data)
    conn.commit()
user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]
user_df
